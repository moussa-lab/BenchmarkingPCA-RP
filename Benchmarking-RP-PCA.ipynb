{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from umap.umap_ import UMAP\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    adjusted_rand_score,\n",
    "    mutual_info_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Spherical KMeans\n",
    "class SphericalKMeans(KMeans):\n",
    "    def __init__(self, n_clusters=2, max_iter=300, random_state=None):\n",
    "        super(SphericalKMeans, self).__init__(n_clusters=n_clusters, max_iter=max_iter, random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        X_normalized = normalize(X, norm='l2')\n",
    "        return super(SphericalKMeans, self).fit(X_normalized, y=y, sample_weight=sample_weight)\n",
    "\n",
    "    def predict(self, X, sample_weight=None):\n",
    "        X_normalized = normalize(X, norm='l2')\n",
    "        return super(SphericalKMeans, self).predict(X_normalized, sample_weight=sample_weight)\n",
    "\n",
    "    def fit_predict(self, X, y=None, sample_weight=None):\n",
    "        X_normalized = normalize(X, norm='l2')\n",
    "        return super(SphericalKMeans, self).fit_predict(X_normalized, y=y, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the dunn index\n",
    "def dunn_index(X, labels):\n",
    "    \n",
    "    # Calculate pairwise cosine distances\n",
    "    distances = squareform(pdist(X, metric='cosine'))\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    intra_cluster_max = 0\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        if len(indices) > 1: \n",
    "            intra_distances = distances[np.ix_(indices, indices)]\n",
    "            np.fill_diagonal(intra_distances, np.nan)  # Ignore self-distances by setting them to NaN\n",
    "            max_intra = np.nanmax(intra_distances)\n",
    "            intra_cluster_max = max(intra_cluster_max, max_intra)\n",
    "\n",
    "    inter_cluster_min = np.inf\n",
    "    for i, label_i in enumerate(unique_labels[:-1]):\n",
    "        for label_j in unique_labels[i + 1:]:\n",
    "            indices_i = np.where(labels == label_i)[0]\n",
    "            indices_j = np.where(labels == label_j)[0]\n",
    "            if len(indices_i) > 0 and len(indices_j) > 0: \n",
    "                inter_distances = distances[np.ix_(indices_i, indices_j)]\n",
    "                min_inter = np.min(inter_distances)\n",
    "                inter_cluster_min = min(inter_cluster_min, min_inter)\n",
    "                \n",
    "    if intra_cluster_max == 0:\n",
    "        intra_cluster_max = np.nan\n",
    "\n",
    "    return inter_cluster_min / intra_cluster_max if intra_cluster_max > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Gap Statistic\n",
    "def gap_statistic(data, refs=None, n_refs=10, k_max=10):\n",
    "\n",
    "    if refs is None:\n",
    "        shape = data.shape\n",
    "        tops = data.max(axis=0)\n",
    "        bottoms = data.min(axis=0)\n",
    "        dists = np.diag(tops - bottoms)\n",
    "        rands = np.random.random_sample(size=(n_refs, shape[0], shape[1]))\n",
    "        refs = rands.dot(dists) + bottoms\n",
    "\n",
    "    gaps = np.zeros(k_max)\n",
    "    for k in range(1, k_max + 1):\n",
    "        # Fit to original data\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        km.fit(data)\n",
    "        Wk = km.inertia_\n",
    "\n",
    "        # Compute Wk for reference data\n",
    "        Wk_refs = np.zeros(n_refs)\n",
    "        for i in range(n_refs):\n",
    "            km_ref = KMeans(n_clusters=k, random_state=42)\n",
    "            km_ref.fit(refs[i])\n",
    "            Wk_refs[i] = km_ref.inertia_\n",
    "\n",
    "        # Compute Gap statistic\n",
    "        gaps[k-1] = np.log(np.mean(Wk_refs)) - np.log(Wk)\n",
    "\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map predicted labels to true labels using Hungarian Algorithm\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    if true_labels is None:\n",
    "        return None\n",
    "    contingency_matrix = pd.crosstab(pd.Series(true_labels, name='True'), pd.Series(predicted_labels, name='Predicted'))\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency_matrix.values)\n",
    "    total = contingency_matrix.values[row_ind, col_ind].sum()\n",
    "    accuracy = total / np.sum(contingency_matrix.values)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute WCSS (Within-Cluster Sum of Squares)\n",
    "# Normalize it /(n-k) to make it comparable\n",
    "def compute_wcss(data, labels):\n",
    "    wcss = 0\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        cluster_points = data[labels == label]\n",
    "        centroid = cluster_points.mean(axis=0)\n",
    "        wcss += np.sum((cluster_points - centroid) ** 2)\n",
    "    return wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform clustering using Spherical KMeans and evaluate metrics\n",
    "def cluster_and_evaluate(data, true_labels, n_clusters, labeled):\n",
    "    # Initialize Spherical KMeans\n",
    "    spherical_kmeans = SphericalKMeans(n_clusters=n_clusters, random_state=42)\n",
    "    predicted_labels = spherical_kmeans.fit_predict(data)\n",
    "    \n",
    "    metrics = {}\n",
    "    if labeled:\n",
    "        # Calculate accuracy and mutual information if labels are available\n",
    "        accuracy = calculate_accuracy(true_labels, predicted_labels)\n",
    "        mutual_info = mutual_info_score(true_labels, predicted_labels)\n",
    "        metrics['Accuracy'] = accuracy\n",
    "        metrics['Mutual Information'] = mutual_info\n",
    "        print(f\"Clustering Accuracy: {accuracy:.4f}, Mutual Information: {mutual_info:.4f}\")\n",
    "        # Calculate Dunn index and Adjusted Rand Index\n",
    "        dunn = dunn_index(data, predicted_labels)\n",
    "        ari = adjusted_rand_score(true_labels, predicted_labels) if true_labels is not None else None\n",
    "        metrics['Dunn Index'] = dunn\n",
    "        metrics['Adjusted Rand Index'] = ari\n",
    "        print(f\"Dunn Index: {dunn:.4f}, Adjusted Rand Index: {ari:.4f}\")\n",
    "\n",
    "    return predicted_labels, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot UMAP results\n",
    "def plot_umap_results(results, labels, title, color_palette, folder=\"plots\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(results[:, 0], results[:, 1], c=labels, cmap=color_palette, s=50, edgecolor='k')\n",
    "        plt.colorbar(scatter, label='Label Category')\n",
    "    else:\n",
    "        scatter = plt.scatter(results[:, 0], results[:, 1], cmap=color_palette, s=50, edgecolor='k')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f\"{folder}/{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def plot_wcss_boxplot(results_df, dataset_name, metrics_folder, base_palette):\n",
    "    methods = results_df['Reduction Method'].unique()\n",
    "    clustering_methods = results_df['Clustering Method'].unique()\n",
    "    \n",
    "    # Check if WCSS is present in the results\n",
    "    if 'WCSS' not in results_df.columns or results_df['WCSS'].isnull().all():\n",
    "        print(\"WCSS metric is missing or has no valid data.\")\n",
    "        return\n",
    "\n",
    "    # Define broader bins for the components\n",
    "    def categorize_components(x):\n",
    "        if 5 <= x <= 25:\n",
    "            return 'Small Components'\n",
    "        elif 50 <= x <= 250:\n",
    "            return 'Medium Components'\n",
    "        elif x >= 275:\n",
    "            return 'Large Components'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "    # Apply categorization\n",
    "    results_df['Component Category'] = results_df['Components'].apply(categorize_components)\n",
    "\n",
    "    # Generate color palette for the component categories\n",
    "    component_categories = results_df['Component Category'].unique()\n",
    "    palette = sns.color_palette('husl', n_colors=len(component_categories))\n",
    "    component_colors = dict(zip(component_categories, palette))\n",
    "    \n",
    "    # Plotting WCSS as a box plot for each clustering method\n",
    "    for clust_method in clustering_methods:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(\n",
    "            x='Reduction Method',\n",
    "            y='WCSS',\n",
    "            hue='Component Category',\n",
    "            data=results_df[results_df['Clustering Method'].str.strip() == clust_method],\n",
    "            palette=component_colors,\n",
    "            showfliers=False\n",
    "        )\n",
    "        plt.title(f'WCSS Box Plot for {clust_method} - {dataset_name}', fontsize=16)\n",
    "        plt.xlabel('Reduction Method', fontsize=12)\n",
    "        plt.ylabel('WCSS Value', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Component Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = f\"WCSS_Boxplot_{clust_method}_{dataset_name}.png\"\n",
    "        plt.savefig(os.path.join(metrics_folder, plot_filename), bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot clustering metrics and save plots\n",
    "def plot_metrics_and_timing(results_df, timing_df, dataset_name, metrics_folder, timing_folder, color_palette):\n",
    "    methods = results_df['Reduction Method'].unique()\n",
    "    clustering_methods = results_df['Clustering Method'].unique()\n",
    "    \n",
    "    # Determine if the dataset is labeled\n",
    "    if results_df['Adjusted Rand Index'].notnull().any():\n",
    "        labeled = True\n",
    "    else:\n",
    "        labeled = False\n",
    "\n",
    "    # Define metrics based on dataset type\n",
    "    if labeled:\n",
    "        metrics = ['Adjusted Rand Index', 'WCSS' 'Mutual Information', 'Accuracy',\n",
    "                   'Dunn Index', 'Silhouette Score', 'Gap Statistic',\n",
    "                   'Calinski-Harabasz Index', 'Davies-Bouldin Index']\n",
    "    else:\n",
    "        metrics = ['Dunn Index', 'WCSS', 'Silhouette Score', 'Gap Statistic',\n",
    "                   'Calinski-Harabasz Index', 'Davies-Bouldin Index']\n",
    "    \n",
    "    # Generate color palette for Reduction Methods\n",
    "    num_methods = len(methods)\n",
    "    palette = sns.color_palette(color_palette, n_colors=num_methods)\n",
    "    method_colors = dict(zip(methods, palette))\n",
    "    \n",
    "    # Plotting clustering indices with different trend lines separately\n",
    "    for clust_method in clustering_methods:\n",
    "        for index in metrics:\n",
    "            # Check if the metric exists and has valid data\n",
    "            if index not in results_df.columns or results_df[index].isnull().all():\n",
    "                continue\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            for method in methods:\n",
    "                method_data = results_df[(results_df['Reduction Method'].str.strip() == method) & \n",
    "                                         (results_df['Clustering Method'].str.strip() == clust_method)]\n",
    "                if not method_data.empty:\n",
    "                    sns.regplot(\n",
    "                        x='Components', \n",
    "                        y=index, \n",
    "                        data=method_data, \n",
    "                        label=f'{method} Linear Trend', \n",
    "                        marker='o', \n",
    "                        scatter_kws={'s': 50},\n",
    "                        color=method_colors[method]  \n",
    "                    )\n",
    "            plt.title(f'{index} - Linear Trend for {clust_method} - {dataset_name}', fontsize=16)\n",
    "            plt.xlabel('Number of Components', fontsize=12)\n",
    "            plt.ylabel(f'{index} Value', fontsize=12)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plot_filename = f\"{clust_method}_{index}_{dataset_name}_Linear.png\"\n",
    "            plt.savefig(os.path.join(metrics_folder, plot_filename), bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    # Plotting execution times with lines using sns.lineplot (Only Reduction Time)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for method in methods:\n",
    "        method_data = timing_df[timing_df['Reduction Method'].str.strip() == method]\n",
    "        if not method_data.empty:\n",
    "            # Plot Reduction Time\n",
    "            sns.lineplot(\n",
    "                x='Components', \n",
    "                y='Reduction Time', \n",
    "                data=method_data, \n",
    "                label=f'{method} Reduction Time', \n",
    "                marker='o', \n",
    "                linestyle='-', \n",
    "                color=method_colors[method]  # Assign specific color\n",
    "            )\n",
    "    plt.xlabel('Number of Components', fontsize=12)\n",
    "    plt.ylabel('Execution Time (seconds)', fontsize=12)\n",
    "    plt.title(f'Execution Time Comparison for {dataset_name}', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f\"Execution_Times_{dataset_name}_Lines.png\"\n",
    "    plt.savefig(os.path.join(timing_folder, plot_filename), bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot PCA with 2 Components\n",
    "def plot_pca_results(data, labels, title, color_palette, folder=\"plots\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=color_palette, s=50, edgecolor='k')\n",
    "        plt.colorbar(scatter, label='Label Category')\n",
    "    else:\n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], cmap=color_palette, s=50, edgecolor='k')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('PCA Component 1', fontsize=12)\n",
    "    plt.ylabel('PCA Component 2', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f\"{folder}/{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Random Projection results with 2 Components\n",
    "def plot_random_projection_results(data, labels, title, color_palette, projection_type, folder=\"plots\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=color_palette, s=50, edgecolor='k')\n",
    "        plt.colorbar(scatter, label='Label Category')\n",
    "    else:\n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], cmap=color_palette, s=50, edgecolor='k')\n",
    "    plt.title(f'{projection_type} - {title}', fontsize=16)\n",
    "    plt.xlabel(f'{projection_type} Component 1', fontsize=12)\n",
    "    plt.ylabel(f'{projection_type} Component 2', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f\"{folder}/{projection_type}_{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "def plot_dendrogram(Z, **kwargs):\n",
    "    # Plot the dendrogram based on the linkage matrix Z\n",
    "    dendrogram(Z, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single dimensionality reduction and clustering method\n",
    "def process_component(data, method, n_components, n_clusters, pca_type=None, true_labels=None):\n",
    "    if method == 'PCA':\n",
    "        if pca_type == 'full':\n",
    "            reducer = PCA(n_components=n_components, svd_solver='full')\n",
    "            method_label = 'PCA_full'\n",
    "        elif pca_type == 'randomized':\n",
    "            reducer = PCA(n_components=n_components, svd_solver='randomized')\n",
    "            method_label = 'PCA_randomized'\n",
    "    elif method == 'SparseRandomProjection':\n",
    "        reducer = SparseRandomProjection(n_components=n_components, random_state=42)\n",
    "        method_label = 'SparseRandomProjection'\n",
    "    else:  # GaussianRandomProjection\n",
    "        reducer = GaussianRandomProjection(n_components=n_components, random_state=42)\n",
    "        method_label = 'GaussianRandomProjection'\n",
    "        \n",
    "        # Get the log transform - from the beginning try later if we have time\n",
    "\n",
    "    start_time = time.time()\n",
    "    reduced_data = reducer.fit_transform(data)\n",
    "    reduction_time = time.time() - start_time\n",
    "\n",
    "    # Ensure reduced data has same number of rows as original data\n",
    "    assert reduced_data.shape[0] == data.shape[0], \"Mismatch between reduced data and original data!\"\n",
    "    \n",
    "    # Identify zero vectors based on norm\n",
    "    norms = np.linalg.norm(reduced_data, axis=1)\n",
    "    non_zero_indices = norms > 1e-10  # Tolerance to account for floating point precision\n",
    "    num_zero_vectors = np.sum(~non_zero_indices)\n",
    "    if num_zero_vectors > 0:\n",
    "        print(f\"[INFO] Found {num_zero_vectors} zero vectors in {method_label} with {n_components} components. Removing them before clustering.\")\n",
    "        reduced_data = reduced_data[non_zero_indices]\n",
    "        if true_labels is not None:\n",
    "            true_labels = true_labels[non_zero_indices].astype(int)\n",
    "\n",
    "    # Check if enough samples remain for clustering\n",
    "    if reduced_data.shape[0] < n_clusters:\n",
    "        print(f\"[WARNING] Number of samples after removing zero vectors ({reduced_data.shape[0]}) is less than the number of clusters ({n_clusters}). Skipping clustering for {method_label} with {n_components} components.\")\n",
    "        return [], []\n",
    "\n",
    "    results = []\n",
    "    timing_results = []\n",
    "\n",
    "    # Clustering methods\n",
    "    for clustering_method in ['Hierarchical', 'SphericalKMeans']:\n",
    "        if clustering_method == 'Hierarchical':\n",
    "            #clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward', compute_distances=True)\n",
    "            distance_matrix = pdist(reduced_data, metric='cosine')\n",
    "            Z = linkage(distance_matrix, method='ward')\n",
    "            # Extract cluster labels\n",
    "            predicted_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "            #predicted_labels = clusterer.fit_predict(reduced_data)\n",
    "\n",
    "        else:  # SphericalKMeans\n",
    "            clusterer = SphericalKMeans(n_clusters=n_clusters, random_state=42)\n",
    "            predicted_labels = clusterer.fit_predict(reduced_data)\n",
    "\n",
    "        #start_time = time.time()\n",
    "        #predicted_labels = clusterer.fit_predict(reduced_data)\n",
    "        #clustering_time = time.time() - start_time\n",
    "\n",
    "        # Calculate WCSS\n",
    "        wcss = compute_wcss(reduced_data, predicted_labels)\n",
    "\n",
    "        # Calculate metrics\n",
    "        if true_labels is not None:\n",
    "            # Labeled Data Metrics\n",
    "            accuracy = calculate_accuracy(true_labels, predicted_labels)\n",
    "            mutual_info = mutual_info_score(true_labels, predicted_labels)\n",
    "            adjusted_rand = adjusted_rand_score(true_labels, predicted_labels)\n",
    "            gap = gap_statistic(reduced_data, n_refs=10, k_max=n_clusters)[n_clusters-1]  # Gap at k=n_clusters\n",
    "            metrics = {\n",
    "                'Adjusted Rand Index': adjusted_rand,\n",
    "                'Mutual Information': mutual_info,\n",
    "                'Accuracy': accuracy,\n",
    "                'Dunn Index': None,\n",
    "                'Silhouette Score': None,\n",
    "                'Gap Statistic': gap,\n",
    "                'Calinski-Harabasz Index': calinski_harabasz_score(reduced_data, predicted_labels),\n",
    "                'Davies-Bouldin Index': davies_bouldin_score(reduced_data, predicted_labels),\n",
    "                'WCSS': wcss\n",
    "            }\n",
    "        else:\n",
    "            # Unlabeled Data Metrics\n",
    "            dunn = dunn_index(reduced_data, predicted_labels)\n",
    "            silhouette = silhouette_score(reduced_data, predicted_labels)\n",
    "            gap = gap_statistic(reduced_data, n_refs=10, k_max=n_clusters)[n_clusters-1]  # Gap at k=n_clusters\n",
    "            metrics = {\n",
    "                'Adjusted Rand Index': None,\n",
    "                'Mutual Information': None,\n",
    "                'Accuracy': None,\n",
    "                'Dunn Index': dunn,\n",
    "                'Silhouette Score': silhouette,\n",
    "                'Gap Statistic': gap,\n",
    "                'Calinski-Harabasz Index': calinski_harabasz_score(reduced_data, predicted_labels),\n",
    "                'Davies-Bouldin Index': davies_bouldin_score(reduced_data, predicted_labels),\n",
    "                'WCSS': wcss\n",
    "            }\n",
    "\n",
    "        # Append results\n",
    "        results.append({\n",
    "            'Reduction Method': method_label,\n",
    "            'Components': n_components,\n",
    "            'Clustering Method': clustering_method,\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "        # Append timing results (only Reduction Time as per latest request)\n",
    "        timing_results.append({\n",
    "            'Reduction Method': method_label,\n",
    "            'Components': n_components,\n",
    "            'Reduction Time': reduction_time\n",
    "            # 'Clustering Time': clustering_time  # Removed as per latest request\n",
    "        })\n",
    "\n",
    "    return results, timing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments in parallel\n",
    "def run_experiment(data, n_components_range, true_labels=None, n_clusters=2, n_jobs=64):\n",
    "    tasks = [\n",
    "        (method, n_components, pca_type, n_clusters)\n",
    "        for method in ['PCA', 'SparseRandomProjection', 'GaussianRandomProjection']\n",
    "        for n_components in n_components_range\n",
    "        for pca_type in (['full', 'randomized'] if method == 'PCA' else [None])\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    all_timing_results = []\n",
    "\n",
    "    with tqdm(total=len(tasks), desc=\"Processing Tasks\") as pbar:\n",
    "        # Parallel processing using joblib\n",
    "        results = Parallel(n_jobs=n_jobs, backend='loky')(\n",
    "            delayed(process_component)(data, method, n_components, n_clusters, pca_type, true_labels)\n",
    "            for method, n_components, pca_type, n_clusters in tasks\n",
    "        )\n",
    "        for result in results:\n",
    "            all_results.extend(result[0])\n",
    "            all_timing_results.extend(result[1])\n",
    "            pbar.update(1)\n",
    "\n",
    "    return all_results, all_timing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and labels\n",
    "datasets_info = {\n",
    "    # '50-50_Mixture': {\n",
    "    #     'data_path': '../Datasets/Jurkat_Cleaned/Jurkat293T_Clean.csv',\n",
    "    #     'label_path': '../Datasets/Jurkat_Cleaned/Jurkat293T_Clean_TrueLabels.csv',\n",
    "    #     'color_palette': 'viridis',\n",
    "    #     'type': 'labeled',\n",
    "    #     'n_clusters': 2\n",
    "    # },\n",
    "    # 'Labeled_PBMC': {\n",
    "    #     'data_path': '../Datasets/PBMC-Zheng2017/PBMC_SC1.csv',\n",
    "    #     'label_path': '../Datasets/PBMC-Zheng2017/PBMCLabels_SC1ClusterLabels.csv',\n",
    "    #     'color_palette': 'plasma',\n",
    "    #     'type': 'labeled',\n",
    "    #     'n_clusters': 7\n",
    "    # },\n",
    "    # 'Unlabeled_PBMC': {  # Unlabeled\n",
    "    #     'data_path': '../Datasets/Unlabeled_PBMC/unlabled_PBMC.csv',\n",
    "    #     'label_path': None,\n",
    "    #     'color_palette': 'cividis',\n",
    "    #     'type': 'unlabeled',\n",
    "    #     'n_clusters': 6\n",
    "    # },\n",
    "    'Covid19': {  # Unlabeled\n",
    "        'data_path': '../Datasets/Covid19TCells/COVID19DataSC1.csv',\n",
    "        'label_path': None,\n",
    "        '2': 'magma',\n",
    "        'type': 'unlabeled',\n",
    "        'n_clusters': 6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_datasets(datasets_info, n_components_range, output_dir=\"output\"):\n",
    "    # Clear the output directory if it exists\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"Clearing existing output directory: {output_dir}\")\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Created fresh output directory: {output_dir}\")\n",
    "\n",
    "    for dataset_name, info in datasets_info.items():\n",
    "        print(f\"\\n=== Processing Dataset: {dataset_name} ===\")\n",
    "        # Create a unique output subfolder for each dataset\n",
    "        dataset_output_dir = os.path.join(output_dir, dataset_name)\n",
    "        os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "        print(f\"Created directory: {dataset_output_dir}\")\n",
    "\n",
    "        # Create subfolders for different plots\n",
    "        umap_plots_folder = os.path.join(dataset_output_dir, \"UMAP_Plots\")\n",
    "        pca_plots_folder = os.path.join(dataset_output_dir, \"PCA_Plots\")\n",
    "        rp_plots_folder = os.path.join(dataset_output_dir, \"RandomProjection_Plots\")\n",
    "        metrics_plots_folder = os.path.join(dataset_output_dir, \"Metrics_Plots\")\n",
    "        timing_plots_folder = os.path.join(dataset_output_dir, \"Timing_Plots\")\n",
    "        \n",
    "        for folder in [umap_plots_folder, pca_plots_folder, rp_plots_folder, metrics_plots_folder, timing_plots_folder]:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        # Load dataset\n",
    "        try:\n",
    "            dataset = pd.read_csv(info['data_path'], index_col=0)\n",
    "            print(f\"Data Loaded Successfully. Shape: {dataset.shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Data file not found for {dataset_name} at {info['data_path']}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Transpose data to have samples as rows and features as columns\n",
    "        data_transposed = dataset.T.values  # Shape: (samples, features)\n",
    "\n",
    "        # Load labels if available\n",
    "        if info['label_path'] is not None:\n",
    "            try:\n",
    "                labels_df = pd.read_csv(info['label_path'])\n",
    "                # Reindex labels based on dataset.columns to align labels with samples\n",
    "                labels_matched = labels_df.set_index('Unnamed: 0').reindex(dataset.columns)['x'].astype(int).values\n",
    "                # Check for any missing labels after reindexing\n",
    "                if np.isnan(labels_matched).any():\n",
    "                    print(f\"Warning: Some labels are missing for {dataset_name}. Dropping these samples.\")\n",
    "                    valid_indices = ~np.isnan(labels_matched)\n",
    "                    data_transposed = data_transposed[valid_indices]\n",
    "                    labels_matched = labels_matched[valid_indices].astype(int)\n",
    "                print(f\"Labels Loaded Successfully. Shape: {labels_matched.shape}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: Label file not found for {dataset_name} at {info['label_path']}\")\n",
    "                labels_matched = None\n",
    "            except KeyError:\n",
    "                print(f\"Error: 'x' column not found in label file for {dataset_name}.\")\n",
    "                labels_matched = None\n",
    "            except ValueError as ve:\n",
    "                print(f\"Error converting labels to integers for {dataset_name}: {ve}\")\n",
    "                labels_matched = None\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error loading labels for {dataset_name}: {e}\")\n",
    "                labels_matched = None\n",
    "        else:\n",
    "            labels_matched = None\n",
    "\n",
    "        # Run experiment\n",
    "        print(f\"Running dimensionality reduction and clustering for {dataset_name}...\")\n",
    "        try:\n",
    "            results, timing = run_experiment(\n",
    "                data_transposed, \n",
    "                n_components_range, \n",
    "                true_labels=labels_matched, \n",
    "                n_clusters=info['n_clusters']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during experiment for {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        timing_df = pd.DataFrame(timing)\n",
    "\n",
    "        # Save results\n",
    "        results_save_path = os.path.join(dataset_output_dir, f\"{dataset_name}_results.csv\")\n",
    "        timing_save_path = os.path.join(dataset_output_dir, f\"{dataset_name}_timing.csv\")\n",
    "        try:\n",
    "            results_df.to_csv(results_save_path, index=False)\n",
    "            timing_df.to_csv(timing_save_path, index=False)\n",
    "            print(f\"Results saved to {results_save_path} and {timing_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results for {dataset_name}: {e}\")\n",
    "        # Plot WCSS boxplot for this dataset\n",
    "        print(\"Plotting WCSS boxplot for this dataset...\")\n",
    "        try:\n",
    "            plot_wcss_boxplot(\n",
    "                results_df,\n",
    "                dataset_name,\n",
    "                metrics_plots_folder,\n",
    "                info['color_palette']\n",
    "            )\n",
    "            print(f\"WCSS boxplot saved in {metrics_plots_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting WCSS boxplot for {dataset_name}: {e}\")\n",
    "        # Plot clustering metrics and timing for this dataset\n",
    "        print(\"Plotting clustering metrics and execution times for this dataset...\")\n",
    "        try:\n",
    "            plot_metrics_and_timing(\n",
    "                results_df, \n",
    "                timing_df, \n",
    "                dataset_name, \n",
    "                metrics_plots_folder, \n",
    "                timing_plots_folder, \n",
    "                info['color_palette']  # Pass the specific color palette\n",
    "            )\n",
    "            print(f\"Clustering metrics and execution times plots saved in {metrics_plots_folder} and {timing_plots_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting metrics and timing for {dataset_name}: {e}\")\n",
    "            \n",
    "        # Store evaluation results in a dictionary for each dataset\n",
    "        evaluation_results = {\n",
    "            'Method': [],\n",
    "            'Accuracy': [],\n",
    "            'Mutual Information': [],\n",
    "            'Dunn Index': [],\n",
    "            'Adjusted Rand Index': []\n",
    "        }\n",
    "\n",
    "        # Visualization with UMAP, PCA, and Random Projections\n",
    "        print(\"Generating visualizations (UMAP, PCA, Random Projections)...\")\n",
    "        if info['type'] == 'labeled':\n",
    "            try:\n",
    "                # Apply dimensionality reduction to 500 dimensions\n",
    "                pca_full = PCA(n_components=500, svd_solver='full')\n",
    "                pca_full_result = pca_full.fit_transform(data_transposed)\n",
    "                \n",
    "                # Apply dimensionality reduction to 500 dimensions using Randomized PCA\n",
    "                pca_randomized = PCA(n_components=500, svd_solver='randomized', random_state=42)\n",
    "                pca_randomized_result = pca_randomized.fit_transform(data_transposed)\n",
    "\n",
    "                grp = GaussianRandomProjection(n_components=500, random_state=42)\n",
    "                grp_result = grp.fit_transform(data_transposed)\n",
    "\n",
    "                srp = SparseRandomProjection(n_components=500, random_state=42)\n",
    "                srp_result = srp.fit_transform(data_transposed)\n",
    "\n",
    "                # Apply UMAP for visualization\n",
    "                umap_model = UMAP(n_neighbors=5, min_dist=0.3, n_components=3, random_state=42) # `n_components=3` for 3D UMAP\n",
    "                pca_full_umap = umap_model.fit_transform(pca_full_result)\n",
    "                pca_randomized_umap = umap_model.fit_transform(pca_randomized_result)  \n",
    "                grp_umap = umap_model.fit_transform(grp_result)\n",
    "                srp_umap = umap_model.fit_transform(srp_result)\n",
    "                # Perform clustering and evaluation\n",
    "                for method_name, umap_data in zip(\n",
    "                    ['PCA Full', 'PCA Randomized', 'GRP', 'SRP'],\n",
    "                    [pca_full_umap, pca_randomized_umap, grp_umap, srp_umap]\n",
    "                ):\n",
    "                    print(f\"Evaluating {method_name}... for {dataset_name}\")\n",
    "                    predicted_labels, metrics = cluster_and_evaluate(\n",
    "                        umap_data, labels_matched, info['n_clusters'], info['type'] == 'labeled'\n",
    "                    )\n",
    "                    # Collect results for saving\n",
    "                    evaluation_results['Method'].append(method_name)\n",
    "                    evaluation_results['Accuracy'].append(metrics.get('Accuracy'))\n",
    "                    evaluation_results['Mutual Information'].append(metrics.get('Mutual Information'))\n",
    "                    evaluation_results['Dunn Index'].append(metrics.get('Dunn Index'))\n",
    "                    evaluation_results['Adjusted Rand Index'].append(metrics.get('Adjusted Rand Index'))\n",
    "                # Convert evaluation results to DataFrame\n",
    "                evaluation_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "                # Save the evaluation results as a CSV file\n",
    "                evaluation_save_path = os.path.join(dataset_output_dir, f\"{dataset_name}_evaluation.csv\")\n",
    "                evaluation_df.to_csv(evaluation_save_path, index=False)\n",
    "                print(f\"Evaluation results saved to {evaluation_save_path}\")\n",
    "\n",
    "                # Plot UMAP results\n",
    "                plot_umap_results(\n",
    "                    pca_full_umap, \n",
    "                    labels_matched, \n",
    "                    f'UMAP PCA Full - {dataset_name}', \n",
    "                    info['color_palette'], \n",
    "                    folder=umap_plots_folder\n",
    "                )\n",
    "                \n",
    "                # Plot UMAP results for Randomized PCA (New)\n",
    "                plot_umap_results(\n",
    "                    pca_randomized_umap, \n",
    "                    labels_matched, \n",
    "                    f'UMAP PCA Randomized - {dataset_name}', \n",
    "                    info['color_palette'], \n",
    "                    folder=umap_plots_folder\n",
    "                )\n",
    "                \n",
    "                plot_umap_results(\n",
    "                    grp_umap, \n",
    "                    labels_matched, \n",
    "                    f'UMAP GRP - {dataset_name}', \n",
    "                    info['color_palette'], \n",
    "                    folder=umap_plots_folder\n",
    "                )\n",
    "                plot_umap_results(\n",
    "                    srp_umap, \n",
    "                    labels_matched, \n",
    "                    f'UMAP SRP - {dataset_name}', \n",
    "                    info['color_palette'], \n",
    "                    folder=umap_plots_folder\n",
    "                )\n",
    "                print(f\"UMAP plots saved in {umap_plots_folder}\")\n",
    "                \n",
    "                \n",
    "\n",
    "                # Plot PCA with 2 Components\n",
    "                pca_2d = PCA(n_components=2, random_state=42).fit_transform(data_transposed)\n",
    "                plot_pca_results(\n",
    "                    pca_2d, \n",
    "                    labels_matched, \n",
    "                    f'PCA 2D - {dataset_name}', \n",
    "                    info['color_palette'], \n",
    "                    folder=pca_plots_folder\n",
    "                )\n",
    "                print(f\"PCA plots saved in {pca_plots_folder}\")\n",
    "\n",
    "                # Plot Gaussian Random Projection with 2 Components\n",
    "                grp_2d = GaussianRandomProjection(n_components=2, random_state=42).fit_transform(data_transposed)\n",
    "                plot_random_projection_results(\n",
    "                    grp_2d, \n",
    "                    labels_matched, \n",
    "                    f'GRP 2D - {dataset_name}', \n",
    "                    info['color_palette'], \n",
    "                    'GRP', \n",
    "                    folder=rp_plots_folder\n",
    "                )\n",
    "                print(f\"Gaussian Random Projection plots saved in {rp_plots_folder}\")\n",
    "\n",
    "                # Plot Sparse Random Projection with 2 Components\n",
    "                srp_2d = SparseRandomProjection(n_components=2, random_state=42).fit_transform(data_transposed)\n",
    "                plot_random_projection_results(\n",
    "                    srp_2d, \n",
    "                    labels_matched, \n",
    "                    f'SRP 2D - {dataset_name}', \n",
    "                    info['color_palette'], \n",
    "                    'SRP', \n",
    "                    folder=rp_plots_folder\n",
    "                )\n",
    "                print(f\"Sparse Random Projection plots saved in {rp_plots_folder}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during visualization for {dataset_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping visualization for {dataset_name} as it is an unlabeled dataset.\")\n",
    "\n",
    "        print(f\"=== Finished Processing Dataset: {dataset_name} ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directory: ./output\n",
      "Created fresh output directory: ./output\n",
      "\n",
      "=== Processing Dataset: Covid19 ===\n",
      "Created directory: ./output/Covid19\n",
      "Data Loaded Successfully. Shape: (12045, 7158)\n",
      "Running dimensionality reduction and clustering for Covid19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Tasks: 100%|██████████| 236/236 [29:40<00:00,  7.55s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./output/Covid19/Covid19_results.csv and ./output/Covid19/Covid19_timing.csv\n",
      "Plotting WCSS boxplot for this dataset...\n",
      "WCSS boxplot saved in ./output/Covid19/Metrics_Plots\n",
      "Plotting clustering metrics and execution times for this dataset...\n",
      "Clustering metrics and execution times plots saved in ./output/Covid19/Metrics_Plots and ./output/Covid19/Timing_Plots\n",
      "Generating visualizations (UMAP, PCA, Random Projections)...\n",
      "Skipping visualization for Covid19 as it is an unlabeled dataset.\n",
      "=== Finished Processing Dataset: Covid19 ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Range for number of components\n",
    "    n_components_range = list(range(5, 24, 1))  + list(range(25, 1001, 25)) \n",
    "    #n_components_range = list(range(5, 25, 5))\n",
    "\n",
    "    # Output directory\n",
    "    output_directory = \"./output\"\n",
    "\n",
    "    # Process all datasets\n",
    "    process_all_datasets(datasets_info, n_components_range, output_dir=output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WCSS DO BOX PLOT\n",
    "Locality - Do clustering after th\n",
    "small table \n",
    "Fix hieriecal clustering - ward with cosine distance - ward with ecludian\n",
    "box plot -variability\n",
    "locality clustering results - get a small table\n",
    "\n",
    "The legend inside for the boxplot and fix the \n",
    "\n",
    "Fix the title and the legends for all\n",
    "\n",
    "Check mutual information\n",
    "Upload "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mohamed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
