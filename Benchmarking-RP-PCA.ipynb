{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from umap.umap_ import UMAP\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    mutual_info_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Spherical KMeans\n",
    "class SphericalKMeans(KMeans):\n",
    "    def __init__(self, n_clusters=2, max_iter=300, random_state=None):\n",
    "        super(SphericalKMeans, self).__init__(n_clusters=n_clusters, max_iter=max_iter, random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        X_normalized = normalize(X, norm='l2')\n",
    "        return super(SphericalKMeans, self).fit(X_normalized, y=y, sample_weight=sample_weight)\n",
    "\n",
    "    def predict(self, X, sample_weight=None):\n",
    "        X_normalized = normalize(X, norm='l2')\n",
    "        return super(SphericalKMeans, self).predict(X_normalized, sample_weight=sample_weight)\n",
    "\n",
    "    def fit_predict(self, X, y=None, sample_weight=None):\n",
    "        X_normalized = normalize(X, norm='l2')\n",
    "        return super(SphericalKMeans, self).fit_predict(X_normalized, y=y, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the dunn index\n",
    "def dunn_index(X, labels):\n",
    "    \n",
    "    # Calculate pairwise cosine distances\n",
    "    distances = squareform(pdist(X, metric='cosine'))\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    intra_cluster_max = 0\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        if len(indices) > 1: \n",
    "            intra_distances = distances[np.ix_(indices, indices)]\n",
    "            np.fill_diagonal(intra_distances, np.nan)  # Ignore self-distances by setting them to NaN\n",
    "            max_intra = np.nanmax(intra_distances)\n",
    "            intra_cluster_max = max(intra_cluster_max, max_intra)\n",
    "\n",
    "    inter_cluster_min = np.inf\n",
    "    for i, label_i in enumerate(unique_labels[:-1]):\n",
    "        for label_j in unique_labels[i + 1:]:\n",
    "            indices_i = np.where(labels == label_i)[0]\n",
    "            indices_j = np.where(labels == label_j)[0]\n",
    "            if len(indices_i) > 0 and len(indices_j) > 0: \n",
    "                inter_distances = distances[np.ix_(indices_i, indices_j)]\n",
    "                min_inter = np.min(inter_distances)\n",
    "                inter_cluster_min = min(inter_cluster_min, min_inter)\n",
    "                \n",
    "    if intra_cluster_max == 0:\n",
    "        intra_cluster_max = np.nan\n",
    "\n",
    "    return inter_cluster_min / intra_cluster_max if intra_cluster_max > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Gap Statistic\n",
    "def gap_statistic(data, refs=None, n_refs=10, k_max=10):\n",
    "\n",
    "    if refs is None:\n",
    "        shape = data.shape\n",
    "        tops = data.max(axis=0)\n",
    "        bottoms = data.min(axis=0)\n",
    "        dists = np.diag(tops - bottoms)\n",
    "        rands = np.random.random_sample(size=(n_refs, shape[0], shape[1]))\n",
    "        refs = rands.dot(dists) + bottoms\n",
    "\n",
    "    gaps = np.zeros(k_max)\n",
    "    for k in range(1, k_max + 1):\n",
    "        # Fit to original data\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        km.fit(data)\n",
    "        Wk = km.inertia_\n",
    "\n",
    "        # Compute Wk for reference data\n",
    "        Wk_refs = np.zeros(n_refs)\n",
    "        for i in range(n_refs):\n",
    "            km_ref = KMeans(n_clusters=k, random_state=42)\n",
    "            km_ref.fit(refs[i])\n",
    "            Wk_refs[i] = km_ref.inertia_\n",
    "\n",
    "        # Compute Gap statistic\n",
    "        gaps[k-1] = np.log(np.mean(Wk_refs)) - np.log(Wk)\n",
    "\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map predicted labels to true labels using Hungarian Algorithm\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    if true_labels is None:\n",
    "        return None\n",
    "    contingency_matrix = pd.crosstab(pd.Series(true_labels, name='True'), pd.Series(predicted_labels, name='Predicted'))\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency_matrix.values)\n",
    "    total = contingency_matrix.values[row_ind, col_ind].sum()\n",
    "    accuracy = total / np.sum(contingency_matrix.values)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute WCSS (Within-Cluster Sum of Squares)\n",
    "def compute_wcss(data, labels):\n",
    "    wcss = 0\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        cluster_points = data[labels == label]\n",
    "        centroid = cluster_points.mean(axis=0)\n",
    "        wcss += np.sum((cluster_points - centroid) ** 2)\n",
    "    return wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform clustering using Spherical KMeans and evaluate metrics\n",
    "def cluster_and_evaluate(data, true_labels, n_clusters, labeled):\n",
    "    # Initialize Spherical KMeans\n",
    "    spherical_kmeans = SphericalKMeans(n_clusters=n_clusters, random_state=42)\n",
    "    predicted_labels = spherical_kmeans.fit_predict(data)\n",
    "    \n",
    "    metrics = {}\n",
    "    if labeled:\n",
    "        # Calculate accuracy and mutual information if labels are available\n",
    "        accuracy = calculate_accuracy(true_labels, predicted_labels)\n",
    "        mutual_info = mutual_info_score(true_labels, predicted_labels)\n",
    "        metrics['Accuracy'] = accuracy\n",
    "        metrics['Mutual Information'] = mutual_info\n",
    "        print(f\"Clustering Accuracy: {accuracy:.4f}, Mutual Information: {mutual_info:.4f}\")\n",
    "        # Calculate Dunn index and Adjusted Rand Index\n",
    "        dunn = dunn_index(data, predicted_labels)\n",
    "        ari = adjusted_rand_score(true_labels, predicted_labels) if true_labels is not None else None\n",
    "        metrics['Dunn Index'] = dunn\n",
    "        metrics['Adjusted Rand Index'] = ari\n",
    "        print(f\"Dunn Index: {dunn:.4f}, Adjusted Rand Index: {ari:.4f}\")\n",
    "\n",
    "    return predicted_labels, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot UMAP results\n",
    "def plot_umap_results(results, labels, title, color_palette, folder=\"plots\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(results[:, 0], results[:, 1], c=labels, cmap=color_palette, s=50, edgecolor='k')\n",
    "        plt.colorbar(scatter, label='Label Category')\n",
    "    else:\n",
    "        scatter = plt.scatter(results[:, 0], results[:, 1], cmap=color_palette, s=50, edgecolor='k')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f\"{folder}/{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wcss_boxplot(results_df, dataset_name, metrics_folder, base_palette):\n",
    "    methods = results_df['Reduction Method'].unique()\n",
    "    clustering_methods = results_df['Clustering Method'].unique()\n",
    "    \n",
    "    # Check if WCSS is present in the results\n",
    "    if 'WCSS' not in results_df.columns or results_df['WCSS'].isnull().all():\n",
    "        print(\"WCSS metric is missing or has no valid data.\")\n",
    "        return\n",
    "\n",
    "    # Define broader bins for the components\n",
    "    def categorize_components(x):\n",
    "        if 5 <= x < 25:\n",
    "            return 'Components < 25'\n",
    "        else: \n",
    "            return 'Components >= 25'\n",
    "\n",
    "    # Apply categorization\n",
    "    results_df['Component Category'] = results_df['Components'].apply(categorize_components)\n",
    "\n",
    "    # Generate color palette for the component categories\n",
    "    component_categories = results_df['Component Category'].unique()\n",
    "    palette = sns.color_palette('husl', n_colors=len(component_categories))\n",
    "    component_colors = dict(zip(component_categories, palette))\n",
    "    \n",
    "    # Plotting WCSS as a box plot for each clustering method\n",
    "    for clust_method in clustering_methods:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(\n",
    "            x='Reduction Method',\n",
    "            y='WCSS',\n",
    "            hue='Component Category',\n",
    "            data=results_df[results_df['Clustering Method'].str.strip() == clust_method],\n",
    "            palette=component_colors,\n",
    "            showfliers=False\n",
    "        )\n",
    "        plt.title(f'WCSS Box Plot for {clust_method} - {dataset_name}', fontsize=16)\n",
    "        plt.xlabel('Dimensionality Reduction Method', fontsize=12)\n",
    "        plt.ylabel('WCSS Value', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Position the legend inside the plot\n",
    "        plt.legend(\n",
    "            title='Component Category',\n",
    "            loc='lower right',\n",
    "            bbox_to_anchor=(1.0, 0.0),\n",
    "            framealpha=0.5,\n",
    "            borderaxespad=0.1\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_filename = f\"WCSS_Boxplot_{clust_method}_{dataset_name}.png\"\n",
    "        plt.savefig(os.path.join(metrics_folder, plot_filename), bbox_inches='tight', dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot clustering metrics and save plots\n",
    "def plot_metrics_and_timing(results_df, timing_df, dataset_name, metrics_folder, timing_folder, metrics_palette, timing_palette, marker_style):\n",
    "    methods = results_df['Reduction Method'].unique()\n",
    "    clustering_methods = results_df['Clustering Method'].unique()\n",
    "    \n",
    "    # Determine if the dataset is labeled\n",
    "    if results_df['Adjusted Rand Index'].notnull().any():\n",
    "        labeled = True\n",
    "    else:\n",
    "        labeled = False\n",
    "\n",
    "    # Define metrics based on dataset type\n",
    "    if labeled:\n",
    "        metrics = ['Adjusted Rand Index', 'WCSS', 'Mutual Information', 'Accuracy',\n",
    "                'Dunn Index', 'Silhouette Score', 'Gap Statistic',\n",
    "                'Calinski-Harabasz Index', 'Davies-Bouldin Index']\n",
    "    else:\n",
    "        metrics = ['Dunn Index', 'WCSS', 'Silhouette Score', 'Gap Statistic',\n",
    "                'Calinski-Harabasz Index', 'Davies-Bouldin Index']\n",
    "    \n",
    "    # Generate color palettes for methods\n",
    "    num_methods = len(methods)\n",
    "    metrics_palette_colors = sns.color_palette(metrics_palette, n_colors=num_methods)\n",
    "    metrics_method_colors = dict(zip(methods, metrics_palette_colors))\n",
    "    # Use the custom timing palette provided\n",
    "    timing_palette_colors = timing_palette\n",
    "    timing_method_colors = dict(zip(methods, timing_palette_colors))\n",
    "    \n",
    "    # Plotting clustering indices with different trend lines separately\n",
    "    for clust_method in clustering_methods:\n",
    "        for index in metrics:\n",
    "            # Check if the metric exists and has valid data\n",
    "            if index not in results_df.columns or results_df[index].isnull().all():\n",
    "                continue\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            for method in methods:\n",
    "                method_data = results_df[(results_df['Reduction Method'].str.strip() == method) & \n",
    "                                        (results_df['Clustering Method'].str.strip() == clust_method)]\n",
    "                if not method_data.empty:\n",
    "                    sns.regplot(\n",
    "                        x='Components', \n",
    "                        y=index, \n",
    "                        data=method_data, \n",
    "                        label=f'{method} Linear Trend', \n",
    "                        marker=marker_style, \n",
    "                        scatter_kws={'s': 50},\n",
    "                        color=metrics_method_colors[method]  \n",
    "                    )\n",
    "            plt.title(f'{index} - Linear Trend for {clust_method} - {dataset_name}', fontsize=16)\n",
    "            plt.xlabel('Number of Components', fontsize=12)\n",
    "            plt.ylabel(f'{index} Value', fontsize=12)\n",
    "            plt.legend()\n",
    "            # Cap accuracy at 1\n",
    "            if index == 'Accuracy':\n",
    "                plt.ylim(0, 1)\n",
    "            plt.tight_layout()\n",
    "            plot_filename = f\"{clust_method}_{index}_{dataset_name}_Linear.png\"\n",
    "            plt.savefig(os.path.join(metrics_folder, plot_filename), bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    # Plotting execution times with trend lines using sns.regplot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for method in methods:\n",
    "        method_data = timing_df[timing_df['Reduction Method'].str.strip() == method]\n",
    "        if not method_data.empty:\n",
    "            sns.regplot(\n",
    "                x='Components', \n",
    "                y='Reduction Time', \n",
    "                data=method_data, \n",
    "                label=f'{method} Reduction Time', \n",
    "                marker=marker_style,  \n",
    "                scatter_kws={'s': 50},\n",
    "                color=timing_method_colors[method]\n",
    "            )\n",
    "    plt.xlabel('Number of Components', fontsize=12)\n",
    "    plt.ylabel('Execution Time (seconds)', fontsize=12)\n",
    "    plt.title(f'Execution Time Comparison for {dataset_name}', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f\"Execution_Times_{dataset_name}_Trend.png\"\n",
    "    plt.savefig(os.path.join(timing_folder, plot_filename), bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot PCA with 2 Components\n",
    "def plot_pca_results(data, labels, title, color_palette, folder=\"plots\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=color_palette, s=50, edgecolor='k')\n",
    "        plt.colorbar(scatter, label='Label Category')\n",
    "    else:\n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], cmap=color_palette, s=50, edgecolor='k')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('PCA Component 1', fontsize=12)\n",
    "    plt.ylabel('PCA Component 2', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f\"{folder}/{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Random Projection results with 2 Components\n",
    "def plot_random_projection_results(data, labels, title, color_palette, projection_type, folder=\"plots\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=color_palette, s=50, edgecolor='k')\n",
    "        plt.colorbar(scatter, label='Label Category')\n",
    "    else:\n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], cmap=color_palette, s=50, edgecolor='k')\n",
    "    plt.title(f'{title}', fontsize=16)\n",
    "    plt.xlabel(f'{projection_type} Component 1', fontsize=12)\n",
    "    plt.ylabel(f'{projection_type} Component 2', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f\"{folder}/{projection_type}_{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "def plot_dendrogram(Z, **kwargs):\n",
    "    # Plot the dendrogram based on the linkage matrix Z\n",
    "    dendrogram(Z, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single dimensionality reduction and clustering method\n",
    "def process_component(data, method, n_components, n_clusters, pca_type=None, true_labels=None):\n",
    "    if method == 'PCA':\n",
    "        if pca_type == 'full':\n",
    "            reducer = PCA(n_components=n_components, svd_solver='full')\n",
    "            method_label = 'PCA_full'\n",
    "        elif pca_type == 'randomized':\n",
    "            reducer = PCA(n_components=n_components, svd_solver='randomized')\n",
    "            method_label = 'PCA_randomized'\n",
    "    elif method == 'SparseRandomProjection':\n",
    "        reducer = SparseRandomProjection(n_components=n_components, random_state=42)\n",
    "        method_label = 'SparseRandomProjection'\n",
    "    else:  # GaussianRandomProjection\n",
    "        reducer = GaussianRandomProjection(n_components=n_components, random_state=42)\n",
    "        method_label = 'GaussianRandomProjection'\n",
    "        \n",
    "    # Perform dimensionality reduction    \n",
    "    start_time = time.time()\n",
    "    reduced_data = reducer.fit_transform(data)\n",
    "    reduction_time = time.time() - start_time\n",
    "\n",
    "    # Ensure reduced data has same number of rows as original data\n",
    "    assert reduced_data.shape[0] == data.shape[0], \"Mismatch between reduced data and original data!\"\n",
    "    \n",
    "    # Identify zero vectors based on norm\n",
    "    norms = np.linalg.norm(reduced_data, axis=1)\n",
    "    non_zero_indices = norms > 1e-10  # Tolerance to account for floating point precision\n",
    "    num_zero_vectors = np.sum(~non_zero_indices)\n",
    "    if num_zero_vectors > 0:\n",
    "        print(f\"[INFO] Found {num_zero_vectors} zero vectors in {method_label} with {n_components} components. Removing them before clustering.\")\n",
    "        reduced_data = reduced_data[non_zero_indices]\n",
    "        if true_labels is not None:\n",
    "            true_labels = true_labels[non_zero_indices].astype(int)\n",
    "\n",
    "    # Check if enough samples remain for clustering\n",
    "    if reduced_data.shape[0] < n_clusters:\n",
    "        print(f\"[WARNING] Number of samples after removing zero vectors ({reduced_data.shape[0]}) is less than the number of clusters ({n_clusters}). Skipping clustering for {method_label} with {n_components} components.\")\n",
    "        return [], []\n",
    "\n",
    "    results = []\n",
    "    timing_results = []\n",
    "\n",
    "    # Clustering methods\n",
    "    for clustering_method in ['Hierarchical', 'SphericalKMeans']:\n",
    "        if clustering_method == 'Hierarchical':\n",
    "            #clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward', compute_distances=True)\n",
    "            distance_matrix = pdist(reduced_data, metric='cosine')\n",
    "            Z = linkage(distance_matrix, method='ward')\n",
    "            # Extract cluster labels\n",
    "            predicted_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "            #predicted_labels = clusterer.fit_predict(reduced_data)\n",
    "\n",
    "        else:  # SphericalKMeans\n",
    "            clusterer = SphericalKMeans(n_clusters=n_clusters, random_state=42)\n",
    "            predicted_labels = clusterer.fit_predict(reduced_data)\n",
    "\n",
    "        #start_time = time.time()\n",
    "        #predicted_labels = clusterer.fit_predict(reduced_data)\n",
    "        #clustering_time = time.time() - start_time\n",
    "\n",
    "        # Calculate WCSS\n",
    "        wcss = compute_wcss(reduced_data, predicted_labels)\n",
    "\n",
    "        # Calculate metrics\n",
    "        if true_labels is not None:\n",
    "            # Labeled Data Metrics\n",
    "            accuracy = calculate_accuracy(true_labels, predicted_labels)\n",
    "            mutual_info = mutual_info_score(true_labels, predicted_labels)\n",
    "            adjusted_rand = adjusted_rand_score(true_labels, predicted_labels)\n",
    "            gap = gap_statistic(reduced_data, n_refs=10, k_max=n_clusters)[n_clusters-1]  # Gap at k=n_clusters\n",
    "            metrics = {\n",
    "                'Mutual Information': mutual_info,\n",
    "                'Accuracy': accuracy,\n",
    "                'Dunn Index': None,\n",
    "                'Gap Statistic': gap,\n",
    "                'WCSS': wcss\n",
    "            }\n",
    "        else:\n",
    "            # Unlabeled Data Metrics\n",
    "            dunn = dunn_index(reduced_data, predicted_labels)\n",
    "            gap = gap_statistic(reduced_data, n_refs=10, k_max=n_clusters)[n_clusters-1]  # Gap at k=n_clusters\n",
    "            metrics = {\n",
    "                'Mutual Information': None,\n",
    "                'Accuracy': None,\n",
    "                'Dunn Index': dunn,\n",
    "                'Gap Statistic': gap,\n",
    "                'WCSS': wcss\n",
    "            }\n",
    "\n",
    "        # Append results\n",
    "        results.append({\n",
    "            'Reduction Method': method_label,\n",
    "            'Components': n_components,\n",
    "            'Clustering Method': clustering_method,\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "        # Append timing results\n",
    "        timing_results.append({\n",
    "            'Reduction Method': method_label,\n",
    "            'Components': n_components,\n",
    "            'Reduction Time': reduction_time\n",
    "        })\n",
    "\n",
    "    return results, timing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments in parallel\n",
    "def run_experiment(data, n_components_range, true_labels=None, n_clusters=2, n_jobs=64):\n",
    "    tasks = [\n",
    "        (method, n_components, pca_type, n_clusters)\n",
    "        for method in ['PCA', 'SparseRandomProjection', 'GaussianRandomProjection']\n",
    "        for n_components in n_components_range\n",
    "        for pca_type in (['full', 'randomized'] if method == 'PCA' else [None])\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    all_timing_results = []\n",
    "\n",
    "    with tqdm(total=len(tasks), desc=\"Processing Tasks\") as pbar:\n",
    "        # Parallel processing using joblib\n",
    "        results = Parallel(n_jobs=n_jobs, backend='loky')(\n",
    "            delayed(process_component)(data, method, n_components, n_clusters, pca_type, true_labels)\n",
    "            for method, n_components, pca_type, n_clusters in tasks\n",
    "        )\n",
    "        for result in results:\n",
    "            all_results.extend(result[0])\n",
    "            all_timing_results.extend(result[1])\n",
    "            pbar.update(1)\n",
    "\n",
    "    return all_results, all_timing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and labels\n",
    "datasets_info = {\n",
    "    '50-50_Mixture': {\n",
    "        'data_path': '../Datasets/Jurkat_Cleaned/Jurkat293T_Clean.csv',\n",
    "        'label_path': '../Datasets/Jurkat_Cleaned/Jurkat293T_Clean_TrueLabels.csv',\n",
    "        'color_palette': 'viridis',\n",
    "        'type': 'labeled',\n",
    "        'n_clusters': 2\n",
    "     },\n",
    "    'Labeled_PBMC': {\n",
    "        'data_path': '../Datasets/PBMC-Zheng2017/PBMC_SC1.csv',\n",
    "        'label_path': '../Datasets/PBMC-Zheng2017/PBMCLabels_SC1ClusterLabels.csv',\n",
    "        'color_palette': 'plasma',\n",
    "        'type': 'labeled',\n",
    "        'n_clusters': 7\n",
    "    },\n",
    "    'Unlabeled_PBMC': {  # Unlabeled\n",
    "        'data_path': '../Datasets/Unlabeled_PBMC/unlabled_PBMC.csv',\n",
    "        'label_path': None,\n",
    "        'color_palette': 'cividis',\n",
    "        'type': 'unlabeled',\n",
    "        'n_clusters': 6\n",
    "    },\n",
    "    'Covid19': {  # Unlabeled\n",
    "        'data_path': '../Datasets/Covid19TCells/COVID19DataSC1.csv',\n",
    "        'label_path': None,\n",
    "        'color_palette': 'magma',\n",
    "        'type': 'unlabeled',\n",
    "        'n_clusters': 6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_datasets(datasets_info, n_components_range, output_dir=\"output\"):\n",
    "    # Define palettes for each section\n",
    "    clustering_metrics_palette = 'magma'\n",
    "    timing_palette = ['blue', 'orange', 'red', 'green']\n",
    "    umap_palette = 'viridis'\n",
    "    \n",
    "    # Define marker styles for datasets\n",
    "    dataset_marker_styles = ['o', 's', 'D', '*', 'v', '<', '>', 'p', '^']\n",
    "    dataset_markers = {}\n",
    "    for dataset_name, marker in zip(datasets_info.keys(), dataset_marker_styles):\n",
    "        dataset_markers[dataset_name] = marker\n",
    "\n",
    "    # Clear the output directory if it exists\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"Clearing existing output directory: {output_dir}\")\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Created fresh output directory: {output_dir}\")\n",
    "\n",
    "    for dataset_name, info in datasets_info.items():\n",
    "        print(f\"\\n=== Processing Dataset: {dataset_name} ===\")\n",
    "        # Create a unique output subfolder for each dataset\n",
    "        dataset_output_dir = os.path.join(output_dir, dataset_name)\n",
    "        os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "        print(f\"Created directory: {dataset_output_dir}\")\n",
    "\n",
    "        # Create subfolders for different plots\n",
    "        umap_plots_folder = os.path.join(dataset_output_dir, \"UMAP_Plots\")\n",
    "        pca_plots_folder = os.path.join(dataset_output_dir, \"PCA_Plots\")\n",
    "        rp_plots_folder = os.path.join(dataset_output_dir, \"RandomProjection_Plots\")\n",
    "        metrics_plots_folder = os.path.join(dataset_output_dir, \"Metrics_Plots\")\n",
    "        timing_plots_folder = os.path.join(dataset_output_dir, \"Timing_Plots\")\n",
    "\n",
    "        for folder in [umap_plots_folder, pca_plots_folder, rp_plots_folder, metrics_plots_folder, timing_plots_folder]:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        # Load dataset\n",
    "        try:\n",
    "            dataset = pd.read_csv(info['data_path'], index_col=0)\n",
    "            print(f\"Data Loaded Successfully. Shape: {dataset.shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Data file not found for {dataset_name} at {info['data_path']}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Transpose data to have samples as rows and features as columns\n",
    "        data_transposed = dataset.T.values  # Shape: (samples, features)\n",
    "\n",
    "        # Load labels if available\n",
    "        if info['label_path'] is not None:\n",
    "            try:\n",
    "                labels_df = pd.read_csv(info['label_path'])\n",
    "                # Reindex labels based on dataset.columns to align labels with samples\n",
    "                labels_matched = labels_df.set_index('Unnamed: 0').reindex(dataset.columns)['x'].astype(int).values\n",
    "                # Check for any missing labels after reindexing\n",
    "                if np.isnan(labels_matched).any():\n",
    "                    print(f\"Warning: Some labels are missing for {dataset_name}. Dropping these samples.\")\n",
    "                    valid_indices = ~np.isnan(labels_matched)\n",
    "                    data_transposed = data_transposed[valid_indices]\n",
    "                    labels_matched = labels_matched[valid_indices].astype(int)\n",
    "                print(f\"Labels Loaded Successfully. Shape: {labels_matched.shape}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: Label file not found for {dataset_name} at {info['label_path']}\")\n",
    "                labels_matched = None\n",
    "            except KeyError:\n",
    "                print(f\"Error: 'x' column not found in label file for {dataset_name}.\")\n",
    "                labels_matched = None\n",
    "            except ValueError as ve:\n",
    "                print(f\"Error converting labels to integers for {dataset_name}: {ve}\")\n",
    "                labels_matched = None\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error loading labels for {dataset_name}: {e}\")\n",
    "                labels_matched = None\n",
    "        else:\n",
    "            labels_matched = None\n",
    "\n",
    "        # Run experiment\n",
    "        print(f\"Running dimensionality reduction and clustering for {dataset_name}...\")\n",
    "        try:\n",
    "            results, timing = run_experiment(\n",
    "                data_transposed, \n",
    "                n_components_range, \n",
    "                true_labels=labels_matched, \n",
    "                n_clusters=info['n_clusters']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during experiment for {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        timing_df = pd.DataFrame(timing)\n",
    "\n",
    "        # Save results\n",
    "        results_save_path = os.path.join(dataset_output_dir, f\"{dataset_name}_results.csv\")\n",
    "        timing_save_path = os.path.join(dataset_output_dir, f\"{dataset_name}_timing.csv\")\n",
    "        try:\n",
    "            results_df.to_csv(results_save_path, index=False)\n",
    "            timing_df.to_csv(timing_save_path, index=False)\n",
    "            print(f\"Results saved to {results_save_path} and {timing_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results for {dataset_name}: {e}\")\n",
    "        # Plot WCSS boxplot for this dataset\n",
    "        print(\"Plotting WCSS boxplot for this dataset...\")\n",
    "        try:\n",
    "            plot_wcss_boxplot(\n",
    "                results_df,\n",
    "                dataset_name,\n",
    "                metrics_plots_folder,\n",
    "                info['color_palette']\n",
    "            )\n",
    "            print(f\"WCSS boxplot saved in {metrics_plots_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting WCSS boxplot for {dataset_name}: {e}\")\n",
    "        # Plot clustering metrics and timing for this dataset\n",
    "        print(\"Plotting clustering metrics and execution times for this dataset...\")\n",
    "        try:\n",
    "            plot_metrics_and_timing(\n",
    "                results_df, \n",
    "                timing_df, \n",
    "                dataset_name, \n",
    "                metrics_plots_folder, \n",
    "                timing_plots_folder, \n",
    "                metrics_palette=clustering_metrics_palette,\n",
    "                timing_palette=timing_palette,\n",
    "                marker_style=dataset_markers[dataset_name]\n",
    "            )\n",
    "            print(f\"Clustering metrics and execution times plots saved in {metrics_plots_folder} and {timing_plots_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting metrics and timing for {dataset_name}: {e}\")\n",
    "                \n",
    "        # Store evaluation results in a dictionary for each dataset\n",
    "        evaluation_results = {\n",
    "            'Method': [],\n",
    "            'Accuracy': [],\n",
    "            'Mutual Information': [],\n",
    "            'Dunn Index': [],\n",
    "            'Adjusted Rand Index': []\n",
    "        }\n",
    "\n",
    "        # Visualization with UMAP, PCA, and Random Projections\n",
    "        print(\"Generating visualizations (UMAP, PCA, Random Projections)...\")\n",
    "        if info['type'] == 'labeled':\n",
    "            try:\n",
    "                # Apply dimensionality reduction to 500 dimensions\n",
    "                pca_full = PCA(n_components=500, svd_solver='full')\n",
    "                pca_full_result = pca_full.fit_transform(data_transposed)\n",
    "                \n",
    "                # Apply dimensionality reduction to 500 dimensions using Randomized PCA\n",
    "                pca_randomized = PCA(n_components=500, svd_solver='randomized', random_state=42)\n",
    "                pca_randomized_result = pca_randomized.fit_transform(data_transposed)\n",
    "\n",
    "                grp = GaussianRandomProjection(n_components=500, random_state=42)\n",
    "                grp_result = grp.fit_transform(data_transposed)\n",
    "\n",
    "                srp = SparseRandomProjection(n_components=500, random_state=42)\n",
    "                srp_result = srp.fit_transform(data_transposed)\n",
    "\n",
    "                # Apply UMAP for visualization\n",
    "                umap_model = UMAP(n_neighbors=5, min_dist=0.3, n_components=3, random_state=42) # `n_components=3` for 3D UMAP\n",
    "                pca_full_umap = umap_model.fit_transform(pca_full_result)\n",
    "                pca_randomized_umap = umap_model.fit_transform(pca_randomized_result)  \n",
    "                grp_umap = umap_model.fit_transform(grp_result)\n",
    "                srp_umap = umap_model.fit_transform(srp_result)\n",
    "                # Perform clustering and evaluation\n",
    "                for method_name, umap_data in zip(\n",
    "                    ['PCA Full', 'PCA Randomized', 'GRP', 'SRP'],\n",
    "                    [pca_full_umap, pca_randomized_umap, grp_umap, srp_umap]\n",
    "                ):\n",
    "                    print(f\"Evaluating {method_name}... for {dataset_name}\")\n",
    "                    predicted_labels, metrics = cluster_and_evaluate(\n",
    "                        umap_data, labels_matched, info['n_clusters'], info['type'] == 'labeled'\n",
    "                    )\n",
    "                    # Collect results for saving\n",
    "                    evaluation_results['Method'].append(method_name)\n",
    "                    evaluation_results['Accuracy'].append(metrics.get('Accuracy'))\n",
    "                    evaluation_results['Mutual Information'].append(metrics.get('Mutual Information'))\n",
    "                    evaluation_results['Dunn Index'].append(metrics.get('Dunn Index'))\n",
    "                    evaluation_results['Adjusted Rand Index'].append(metrics.get('Adjusted Rand Index'))\n",
    "                # Convert evaluation results to DataFrame\n",
    "                evaluation_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "                # Save the evaluation results as a CSV file\n",
    "                evaluation_save_path = os.path.join(dataset_output_dir, f\"{dataset_name}_evaluation.csv\")\n",
    "                evaluation_df.to_csv(evaluation_save_path, index=False)\n",
    "                print(f\"Evaluation results saved to {evaluation_save_path}\")\n",
    "\n",
    "                # Plot UMAP results\n",
    "                plot_umap_results(\n",
    "                    pca_full_umap, \n",
    "                    labels_matched, \n",
    "                    f'PCA Full Locality Preservation - {dataset_name}', \n",
    "                    umap_palette, \n",
    "                    folder=umap_plots_folder\n",
    "                )\n",
    "                \n",
    "                # Plot UMAP results for Randomized PCA (New)\n",
    "                plot_umap_results(\n",
    "                    pca_randomized_umap, \n",
    "                    labels_matched, \n",
    "                    f'PCA Randomized Locality Preservation - {dataset_name}', \n",
    "                    umap_palette, \n",
    "                    folder=umap_plots_folder\n",
    "                )\n",
    "                \n",
    "                plot_umap_results(\n",
    "                    grp_umap, \n",
    "                    labels_matched, \n",
    "                    f'GRP Locality Preservation - {dataset_name}', \n",
    "                    umap_palette, \n",
    "                    folder=umap_plots_folder\n",
    "                )\n",
    "                plot_umap_results(\n",
    "                    srp_umap, \n",
    "                    labels_matched, \n",
    "                    f'SRP Locality Preservation - {dataset_name}', \n",
    "                    umap_palette, \n",
    "                    folder=umap_plots_folder\n",
    "                )\n",
    "                print(f\"UMAP plots saved in {umap_plots_folder}\")\n",
    "                \n",
    "\n",
    "                # Plot PCA with 2 Components\n",
    "                pca_2d = PCA(n_components=2, random_state=42).fit_transform(data_transposed)\n",
    "                plot_pca_results(\n",
    "                    pca_2d, \n",
    "                    labels_matched, \n",
    "                    f'PCA 2D - {dataset_name}', \n",
    "                    umap_palette, \n",
    "                    folder=pca_plots_folder\n",
    "                )\n",
    "                print(f\"PCA plots saved in {pca_plots_folder}\")\n",
    "\n",
    "                # Plot Gaussian Random Projection with 2 Components\n",
    "                grp_2d = GaussianRandomProjection(n_components=2, random_state=42).fit_transform(data_transposed)\n",
    "                plot_random_projection_results(\n",
    "                    grp_2d, \n",
    "                    labels_matched, \n",
    "                    f'GRP 2D - {dataset_name}', \n",
    "                    umap_palette, \n",
    "                    'GRP',\n",
    "                    folder=rp_plots_folder\n",
    "                )\n",
    "                print(f\"Gaussian Random Projection plots saved in {rp_plots_folder}\")\n",
    "\n",
    "                # Plot Sparse Random Projection with 2 Components\n",
    "                srp_2d = SparseRandomProjection(n_components=2, random_state=42).fit_transform(data_transposed)\n",
    "                plot_random_projection_results(\n",
    "                    srp_2d, \n",
    "                    labels_matched, \n",
    "                    f'SRP 2D - {dataset_name}', \n",
    "                    umap_palette, \n",
    "                    'SRP',\n",
    "                    folder=rp_plots_folder\n",
    "                )\n",
    "                print(f\"Sparse Random Projection plots saved in {rp_plots_folder}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during visualization for {dataset_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping visualization for {dataset_name} as it is an unlabeled dataset.\")\n",
    "\n",
    "        print(f\"=== Finished Processing Dataset: {dataset_name} ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca_experiment(data_path, components_list, output_dir, dataset_name):\n",
    "    # Load the dataset\n",
    "    try:\n",
    "        dataset = pd.read_csv(data_path, index_col=0)\n",
    "        print(f\"Data Loaded Successfully for {dataset_name}. Shape: {dataset.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {data_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {dataset_name}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Transpose data to have samples as rows and features as columns\n",
    "    data_transposed = dataset.T.values\n",
    "\n",
    "    # Dictionary to store column sums and PCA components\n",
    "    column_sums = {}\n",
    "    pca_components = {}\n",
    "\n",
    "    for n_components in components_list:\n",
    "        print(f\"Running PCA with {n_components} components for {dataset_name}...\")\n",
    "        try:\n",
    "            # Initialize PCA with full SVD solver\n",
    "            pca = PCA(n_components=n_components, svd_solver='full')\n",
    "            pca_result = pca.fit_transform(data_transposed)\n",
    "\n",
    "            # Calculate column sum\n",
    "            col_sum = pca_result.sum(axis=0)\n",
    "            column_sums[n_components] = col_sum\n",
    "            pca_components[n_components] = pca_result\n",
    "\n",
    "            print(f\"Column sums for {n_components} components: {col_sum}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during PCA for {n_components} components in {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save the column sums to a CSV file\n",
    "    if column_sums:\n",
    "        column_sums_df = pd.DataFrame.from_dict(column_sums, orient='index')\n",
    "        column_sums_df.index.name = 'Components'\n",
    "        output_path_sum = os.path.join(output_dir, f\"{dataset_name}_column_sums.csv\")\n",
    "        column_sums_df.to_csv(output_path_sum)\n",
    "        print(f\"Column sums saved to {output_path_sum}\")\n",
    "\n",
    "    # Save each PCA component set in a separate sheet in an Excel file\n",
    "    if pca_components:\n",
    "        pca_output_path = os.path.join(output_dir, f\"{dataset_name}_pca_components.xlsx\")\n",
    "        with pd.ExcelWriter(pca_output_path) as writer:\n",
    "            for n_components, pca_result in pca_components.items():\n",
    "                pca_df = pd.DataFrame(pca_result)\n",
    "                sheet_name = f\"PCA_{n_components}\"\n",
    "                pca_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        print(f\"PCA components saved to {pca_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths to the PBMC datasets\n",
    "    pbmc_data_paths = {\n",
    "        'Labeled_PBMC': '../Datasets/PBMC-Zheng2017/PBMC_SC1.csv',\n",
    "        'Unlabeled_PBMC': '../Datasets/Unlabeled_PBMC/unlabled_PBMC.csv'\n",
    "    }\n",
    "\n",
    "    # Components to investigate\n",
    "    components_to_test = [200, 400, 600, 800, 1000]\n",
    "\n",
    "    # Output directory for saving results\n",
    "    output_directory = \"./pca_results\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Run the experiment for each dataset\n",
    "    for dataset_name, data_path in pbmc_data_paths.items():\n",
    "        print(f\"\\n=== Running PCA Experiment for {dataset_name} ===\")\n",
    "        run_pca_experiment(data_path, components_to_test, output_directory, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_plot_and_save_combined_column_sums(file_path1, dataset_name1, file_path2, dataset_name2, save_path):\n",
    "    try:\n",
    "        # Load the column sums CSV files\n",
    "        column_sums_df1 = pd.read_csv(file_path1, index_col='Components')\n",
    "        column_sums_df2 = pd.read_csv(file_path2, index_col='Components')\n",
    "        print(f\"Column sums data loaded successfully for {dataset_name1} and {dataset_name2}.\")\n",
    "\n",
    "        # Filter the specific components we are interested in\n",
    "        components_to_plot = [200, 400, 600, 800, 1000]\n",
    "        filtered_df1 = column_sums_df1.loc[components_to_plot]\n",
    "        filtered_df2 = column_sums_df2.loc[components_to_plot]\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Mean across all PCA components for each set of components for both datasets\n",
    "        total_sums1 = filtered_df1.mean(axis=1)\n",
    "        total_sums2 = filtered_df2.mean(axis=1)\n",
    "        \n",
    "        plt.plot(filtered_df1.index, total_sums1, marker='o', label=f'Total Column Mean - {dataset_name1}')\n",
    "        plt.plot(filtered_df2.index, total_sums2, marker='s', linestyle='--', label=f'Total Column Mean - {dataset_name2}')\n",
    "\n",
    "        plt.title('Column Mean vs. Number of Components for Both Datasets', fontsize=16)\n",
    "        plt.xlabel('Number of Components', fontsize=12)\n",
    "        plt.ylabel('Total Column Mean', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Plot saved successfully at {save_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Column sums file not found for one of the datasets.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or plotting data: {e}\")\n",
    "\n",
    "# File paths for the column sums CSV files\n",
    "labeled_pbmc_file = './pca_results/Labeled_PBMC_column_sums.csv'\n",
    "unlabeled_pbmc_file = './pca_results/Unlabeled_PBMC_column_sums.csv'\n",
    "save_plot_path = './pca_results/Combined_Column_Means_Plot.png'\n",
    "\n",
    "# Plotting and saving for both datasets\n",
    "load_and_plot_and_save_combined_column_sums(labeled_pbmc_file, 'Labeled PBMC', unlabeled_pbmc_file, 'Unlabeled PBMC', save_plot_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Range for number of components\n",
    "    n_components_range = list(range(5, 25, 1))  + list(range(25, 1001, 25)) \n",
    "    #n_components_range = list(range(5, 16, 5)) + list(range(50, 101, 50))\n",
    "\n",
    "    # Output directory\n",
    "    output_directory = \"./output\"\n",
    "\n",
    "    # Process all datasets\n",
    "    process_all_datasets(datasets_info, n_components_range, output_dir=output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mohamed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
